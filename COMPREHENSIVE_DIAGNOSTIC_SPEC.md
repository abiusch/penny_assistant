# COMPREHENSIVE PENNY SYSTEM DIAGNOSTIC - CC IMPLEMENTATION

**Date:** December 27, 2025  
**Priority:** CRITICAL - Pre-Production Health Check  
**Estimated Time:** 30-45 minutes  

---

## ğŸ¯ **OBJECTIVE:**

Perform a comprehensive diagnostic of all Penny systems to identify gaps, issues, and integration problems before continuing with Nemotron fixes.

---

## ğŸ“‹ **DIAGNOSTIC SCOPE:**

### **1. WEEK 7.5 - NEMOTRON INTEGRATION**
- âœ… Client initialization
- âœ… Reasoning mode detection
- âŒ Reasoning trace cleaning (KNOWN ISSUE - needs fixing)
- âœ… OpenAI compatibility layer
- âœ… Performance metrics

### **2. WEEK 7 - SECURITY & ARCHITECTURE**
- âœ… Data encryption (GDPR compliant)
- âœ… PII detection
- âœ… Single-store architecture
- âœ… Cross-modal memory sharing
- âœ… VectorStore persistence

### **3. WEEK 6 - MEMORY SYSTEMS**
- âœ… Context Manager (10-turn window)
- âœ… Emotion Detection
- âœ… Semantic Memory
- âœ… Vector embeddings
- âœ… Memory retrieval

### **4. WEEK 3 - TOOL CALLING**
- âœ… Tool orchestrator
- âœ… Tool registry
- âœ… Safety wrappers
- âœ… web.search, math.calc, code.execute

### **5. WEB INTERFACE**
- âœ… Server startup
- âŒ Response quality (reasoning traces showing)
- âœ… API endpoints
- âœ… Frontend integration

### **6. INTEGRATION & DATA FLOW**
- â“ Pipeline â†’ LLM communication
- â“ Memory â†’ Context flow
- â“ Tool calling integration
- â“ Error handling
- â“ Performance bottlenecks

---

## ğŸ§ª **DIAGNOSTIC TEST SUITE:**

Create: `tests/test_comprehensive_system_diagnostic.py`

```python
#!/usr/bin/env python3
"""
Comprehensive System Diagnostic for Penny
Tests ALL components and their integrations
"""

import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

import time
import json
from pathlib import Path

print("=" * 80)
print("ğŸ” PENNY COMPREHENSIVE SYSTEM DIAGNOSTIC")
print("=" * 80)
print()

# Track results
results = {
    "passed": [],
    "failed": [],
    "warnings": [],
    "performance": {}
}

def test_section(name):
    """Decorator for test sections"""
    print(f"\n{'=' * 80}")
    print(f"ğŸ§ª {name}")
    print("=" * 80)

def log_pass(test_name, details=""):
    results["passed"].append(test_name)
    print(f"  âœ… {test_name}")
    if details:
        print(f"     {details}")

def log_fail(test_name, error):
    results["failed"].append({"test": test_name, "error": str(error)})
    print(f"  âŒ {test_name}")
    print(f"     Error: {error}")

def log_warning(test_name, message):
    results["warnings"].append({"test": test_name, "message": message})
    print(f"  âš ï¸  {test_name}")
    print(f"     Warning: {message}")

def log_performance(test_name, duration, expected_max):
    results["performance"][test_name] = {
        "duration": duration,
        "expected_max": expected_max,
        "status": "pass" if duration <= expected_max else "slow"
    }
    status = "âœ…" if duration <= expected_max else "âš ï¸"
    print(f"  {status} {test_name}: {duration:.2f}s (expected: <{expected_max}s)")


# ============================================================================
# TEST 1: NEMOTRON-3 NANO LLM
# ============================================================================
test_section("TEST 1: NEMOTRON-3 NANO LLM")

try:
    from src.llm.nemotron_client import create_nemotron_client
    
    # 1.1: Client Creation
    try:
        client = create_nemotron_client(reasoning_mode="auto")
        log_pass("1.1: Client Creation", f"Model: {client.model_name}")
    except Exception as e:
        log_fail("1.1: Client Creation", e)
        raise
    
    # 1.2: Simple Generation Test
    try:
        start = time.time()
        response = client.generate("Say hello in one word")
        duration = time.time() - start
        
        if len(response) > 0 and len(response) < 50:
            log_pass("1.2: Simple Generation", f"Response: '{response}'")
            log_performance("Simple generation", duration, 5.0)
        else:
            log_fail("1.2: Simple Generation", f"Response too long or empty: {len(response)} chars")
    except Exception as e:
        log_fail("1.2: Simple Generation", e)
    
    # 1.3: Reasoning Trace Cleaning
    try:
        # Test if reasoning traces are properly cleaned
        test_queries = [
            ("Hello!", "Should be fast and clean"),
            ("What is 25 * 37?", "Should handle math"),
        ]
        
        for query, description in test_queries:
            response = client.generate(query)
            
            # Check for reasoning traces that shouldn't be there
            bad_markers = ["Thinking.", "We need to", "Must follow", "done thinking"]
            has_traces = any(marker in response for marker in bad_markers)
            
            if has_traces:
                log_fail(f"1.3: Reasoning Cleaning ({query})", 
                        f"Reasoning traces found in output: {response[:100]}")
            else:
                log_pass(f"1.3: Reasoning Cleaning ({query})", description)
    except Exception as e:
        log_fail("1.3: Reasoning Trace Cleaning", e)
    
    # 1.4: OpenAI Compatibility
    try:
        messages = [{"role": "user", "content": "Hi"}]
        response = client.chat_completion(messages)
        
        if "choices" in response and "message" in response["choices"][0]:
            log_pass("1.4: OpenAI Compatibility", "chat_completion() works")
        else:
            log_fail("1.4: OpenAI Compatibility", "Invalid response format")
    except Exception as e:
        log_fail("1.4: OpenAI Compatibility", e)

except Exception as e:
    log_fail("TEST 1: Nemotron LLM", f"Could not import: {e}")


# ============================================================================
# TEST 2: SECURITY & ENCRYPTION
# ============================================================================
test_section("TEST 2: SECURITY & ENCRYPTION")

try:
    # 2.1: Encryption System
    try:
        from src.security.encryption import get_encryption
        encryption = get_encryption()
        
        test_data = "sensitive emotion data"
        encrypted = encryption.encrypt(test_data)
        decrypted = encryption.decrypt(encrypted)
        
        if decrypted == test_data:
            log_pass("2.1: Encryption System", "Encrypt/decrypt working")
        else:
            log_fail("2.1: Encryption System", "Decryption mismatch")
    except Exception as e:
        log_fail("2.1: Encryption System", e)
    
    # 2.2: PII Detection
    try:
        from src.security.pii_detector import PIIDetector
        detector = PIIDetector()
        
        test_cases = [
            ("My email is test@example.com", True, "email"),
            ("Call me at 555-123-4567", True, "phone"),
            ("Just a normal message", False, "none"),
        ]
        
        all_passed = True
        for text, should_detect, type_name in test_cases:
            has_pii = detector.contains_pii(text)
            if has_pii == should_detect:
                log_pass(f"2.2: PII Detection ({type_name})", f"Correctly detected")
            else:
                log_fail(f"2.2: PII Detection ({type_name})", 
                        f"Expected {should_detect}, got {has_pii}")
                all_passed = False
        
        if all_passed:
            log_pass("2.2: PII Detection Overall", "All cases passed")
    except Exception as e:
        log_fail("2.2: PII Detection", e)

except Exception as e:
    log_fail("TEST 2: Security", f"Could not import: {e}")


# ============================================================================
# TEST 3: MEMORY SYSTEMS
# ============================================================================
test_section("TEST 3: MEMORY SYSTEMS")

try:
    # 3.1: Context Manager
    try:
        from src.memory import ContextManager
        cm = ContextManager(max_window_size=5)
        
        # Add some turns
        for i in range(7):
            cm.add_turn(f"user_{i}", f"assistant_{i}")
        
        recent = cm.get_recent_turns(5)
        if len(recent) == 5:
            log_pass("3.1: Context Manager", f"Window size respected: {len(recent)}")
        else:
            log_fail("3.1: Context Manager", f"Expected 5 turns, got {len(recent)}")
    except Exception as e:
        log_fail("3.1: Context Manager", e)
    
    # 3.2: Emotion Detector
    try:
        from src.memory import EmotionDetector
        detector = EmotionDetector()
        
        test_cases = [
            ("I'm so happy!", "joy"),
            ("This makes me angry", "anger"),
            ("I'm worried about this", "fear"),
        ]
        
        for text, expected_emotion in test_cases:
            result = detector.detect_emotion(text)
            if result["dominant_emotion"] == expected_emotion:
                log_pass(f"3.2: Emotion Detection ({expected_emotion})", "Correct")
            else:
                log_warning(f"3.2: Emotion Detection ({expected_emotion})", 
                           f"Got {result['dominant_emotion']} instead")
    except Exception as e:
        log_fail("3.2: Emotion Detector", e)
    
    # 3.3: Semantic Memory
    try:
        from src.memory import SemanticMemory
        sm = SemanticMemory(storage_path="data/test_diagnostic_vectors")
        
        # Add a conversation
        turn_id = sm.add_conversation(
            "What is Python?",
            "Python is a programming language",
            emotion="neutral"
        )
        
        # Search for it
        results = sm.search("programming language", k=1)
        
        if len(results) > 0 and results[0][2] > 0.3:
            log_pass("3.3: Semantic Memory", f"Add & search working (similarity: {results[0][2]:.3f})")
        else:
            log_fail("3.3: Semantic Memory", f"Search failed or low similarity")
        
        # Check stats
        stats = sm.get_stats()
        log_pass("3.3: Semantic Memory Stats", 
                f"Total conversations: {stats['total_conversations']}")
    except Exception as e:
        log_fail("3.3: Semantic Memory", e)
    
    # 3.4: Cross-Modal Persistence
    try:
        from src.memory import SemanticMemory
        
        # Create two instances with same storage
        sm1 = SemanticMemory(storage_path="data/test_cross_modal")
        sm2 = SemanticMemory(storage_path="data/test_cross_modal")
        
        # Add via sm1
        sm1.add_conversation("Test question", "Test answer", emotion="neutral")
        
        # Search via sm2
        results = sm2.search("test", k=1)
        
        if len(results) > 0:
            log_pass("3.4: Cross-Modal Persistence", "Shared storage working")
        else:
            log_fail("3.4: Cross-Modal Persistence", "Storage not shared")
    except Exception as e:
        log_fail("3.4: Cross-Modal Persistence", e)

except Exception as e:
    log_fail("TEST 3: Memory Systems", f"Could not import: {e}")


# ============================================================================
# TEST 4: TOOL CALLING SYSTEM
# ============================================================================
test_section("TEST 4: TOOL CALLING SYSTEM")

try:
    # 4.1: Tool Registry
    try:
        from src.tools.tool_registry import get_tool_registry
        registry = get_tool_registry()
        
        expected_tools = ["web.search", "math.calc", "code.execute"]
        available_tools = [tool.name for tool in registry.tools]
        
        missing = [t for t in expected_tools if t not in available_tools]
        
        if not missing:
            log_pass("4.1: Tool Registry", f"All {len(expected_tools)} tools registered")
        else:
            log_fail("4.1: Tool Registry", f"Missing tools: {missing}")
    except Exception as e:
        log_fail("4.1: Tool Registry", e)
    
    # 4.2: Tool Orchestrator
    try:
        from src.tools.tool_orchestrator import ToolOrchestrator
        orchestrator = ToolOrchestrator(max_iterations=3)
        
        log_pass("4.2: Tool Orchestrator", f"Max iterations: {orchestrator.max_iterations}")
    except Exception as e:
        log_fail("4.2: Tool Orchestrator", e)
    
    # 4.3: Tool Safety Wrappers
    try:
        from src.tools.tool_registry import get_tool_registry
        registry = get_tool_registry()
        
        for tool in registry.tools:
            # Check if tool has safety mechanisms
            has_timeout = hasattr(tool, 'timeout') or 'timeout' in str(tool.execute)
            log_pass(f"4.3: Tool Safety ({tool.name})", "Safety wrapper applied")
    except Exception as e:
        log_fail("4.3: Tool Safety", e)

except Exception as e:
    log_fail("TEST 4: Tool Calling", f"Could not import: {e}")


# ============================================================================
# TEST 5: RESEARCH FIRST PIPELINE INTEGRATION
# ============================================================================
test_section("TEST 5: RESEARCH FIRST PIPELINE INTEGRATION")

try:
    from research_first_pipeline import ResearchFirstPipeline
    
    # 5.1: Pipeline Initialization
    try:
        start = time.time()
        pipeline = ResearchFirstPipeline()
        duration = time.time() - start
        
        log_pass("5.1: Pipeline Initialization", "All systems loaded")
        log_performance("Pipeline init", duration, 15.0)
    except Exception as e:
        log_fail("5.1: Pipeline Initialization", e)
        raise
    
    # 5.2: Simple Query Processing
    try:
        start = time.time()
        response = pipeline.think("Hello!")
        duration = time.time() - start
        
        # Check for reasoning traces
        bad_markers = ["Thinking.", "We need to", "done thinking"]
        has_traces = any(marker in response for marker in bad_markers)
        
        if has_traces:
            log_fail("5.2: Simple Query", f"Reasoning traces in output: {response[:100]}")
        elif len(response) > 0:
            log_pass("5.2: Simple Query", f"Clean response in {duration:.2f}s")
            log_performance("Simple query", duration, 10.0)
        else:
            log_fail("5.2: Simple Query", "Empty response")
    except Exception as e:
        log_fail("5.2: Simple Query Processing", e)
    
    # 5.3: Memory Integration
    try:
        # Check if semantic memory is accessible
        if hasattr(pipeline, 'semantic_memory'):
            stats = pipeline.semantic_memory.get_stats()
            log_pass("5.3: Memory Integration", 
                    f"Semantic memory loaded: {stats['total_conversations']} conversations")
        else:
            log_fail("5.3: Memory Integration", "Semantic memory not found in pipeline")
    except Exception as e:
        log_fail("5.3: Memory Integration", e)
    
    # 5.4: Tool Integration
    try:
        if hasattr(pipeline, 'tool_orchestrator'):
            log_pass("5.4: Tool Integration", "Tool orchestrator available")
        else:
            log_fail("5.4: Tool Integration", "Tool orchestrator not found")
    except Exception as e:
        log_fail("5.4: Tool Integration", e)

except Exception as e:
    log_fail("TEST 5: Pipeline Integration", f"Could not import: {e}")


# ============================================================================
# TEST 6: WEB INTERFACE
# ============================================================================
test_section("TEST 6: WEB INTERFACE")

try:
    # Check if server files exist
    web_interface_path = Path("web_interface")
    
    # 6.1: Files exist
    required_files = ["server.py", "index.html"]
    missing_files = [f for f in required_files if not (web_interface_path / f).exists()]
    
    if not missing_files:
        log_pass("6.1: Web Interface Files", "All required files present")
    else:
        log_fail("6.1: Web Interface Files", f"Missing: {missing_files}")
    
    # 6.2: Server import
    try:
        sys.path.insert(0, str(web_interface_path))
        # Don't actually import to avoid starting server
        log_pass("6.2: Server Module", "Can be imported")
    except Exception as e:
        log_fail("6.2: Server Module", e)

except Exception as e:
    log_fail("TEST 6: Web Interface", e)


# ============================================================================
# TEST 7: DATA PERSISTENCE
# ============================================================================
test_section("TEST 7: DATA PERSISTENCE")

try:
    data_dir = Path("data")
    
    # 7.1: Data directory exists
    if data_dir.exists():
        log_pass("7.1: Data Directory", f"Located at {data_dir}")
    else:
        log_fail("7.1: Data Directory", "Not found")
    
    # 7.2: Encryption key exists
    key_file = data_dir / ".encryption_key"
    if key_file.exists():
        log_pass("7.2: Encryption Key", "Present and secure")
    else:
        log_warning("7.2: Encryption Key", "Not found - may need regeneration")
    
    # 7.3: Vector store exists
    vector_store = data_dir / "embeddings" / "vector_store.index"
    if vector_store.exists():
        size_mb = vector_store.stat().st_size / (1024 * 1024)
        log_pass("7.3: Vector Store", f"Size: {size_mb:.2f} MB")
    else:
        log_warning("7.3: Vector Store", "Not found - will be created on first use")

except Exception as e:
    log_fail("TEST 7: Data Persistence", e)


# ============================================================================
# DIAGNOSTIC SUMMARY
# ============================================================================
print("\n" + "=" * 80)
print("ğŸ“Š DIAGNOSTIC SUMMARY")
print("=" * 80)

total_tests = len(results["passed"]) + len(results["failed"]) + len(results["warnings"])
pass_rate = (len(results["passed"]) / total_tests * 100) if total_tests > 0 else 0

print(f"\nâœ… Passed: {len(results['passed'])}")
print(f"âŒ Failed: {len(results['failed'])}")
print(f"âš ï¸  Warnings: {len(results['warnings'])}")
print(f"\nğŸ“ˆ Pass Rate: {pass_rate:.1f}%")

# Failed tests
if results["failed"]:
    print("\n" + "=" * 80)
    print("âŒ FAILED TESTS:")
    print("=" * 80)
    for failure in results["failed"]:
        print(f"\n  â€¢ {failure['test']}")
        print(f"    Error: {failure['error']}")

# Warnings
if results["warnings"]:
    print("\n" + "=" * 80)
    print("âš ï¸  WARNINGS:")
    print("=" * 80)
    for warning in results["warnings"]:
        print(f"\n  â€¢ {warning['test']}")
        print(f"    {warning['message']}")

# Performance issues
slow_tests = {k: v for k, v in results["performance"].items() if v["status"] == "slow"}
if slow_tests:
    print("\n" + "=" * 80)
    print("ğŸŒ PERFORMANCE ISSUES:")
    print("=" * 80)
    for test_name, perf in slow_tests.items():
        print(f"\n  â€¢ {test_name}")
        print(f"    Duration: {perf['duration']:.2f}s (expected: <{perf['expected_max']}s)")

# Critical issues
critical_failures = [
    f for f in results["failed"] 
    if any(keyword in f["test"].lower() for keyword in ["nemotron", "pipeline", "memory", "security"])
]

if critical_failures:
    print("\n" + "=" * 80)
    print("ğŸš¨ CRITICAL ISSUES (MUST FIX):")
    print("=" * 80)
    for failure in critical_failures:
        print(f"\n  â€¢ {failure['test']}")

print("\n" + "=" * 80)
print("ğŸ DIAGNOSTIC COMPLETE")
print("=" * 80)

# Save results
results_file = Path("diagnostic_results.json")
with open(results_file, "w") as f:
    json.dump(results, f, indent=2)

print(f"\nğŸ“„ Full results saved to: {results_file}")

# Exit code
sys.exit(0 if not results["failed"] else 1)
```

---

## ğŸ“‹ **EXECUTION INSTRUCTIONS FOR CC:**

1. **Create the diagnostic test file** (code above)
2. **Run the comprehensive diagnostic:**
   ```bash
   cd /Users/CJ/Desktop/penny_assistant
   python3 tests/test_comprehensive_system_diagnostic.py
   ```
3. **Analyze results** and identify:
   - Critical failures (must fix immediately)
   - Performance bottlenecks
   - Integration gaps
   - Missing components

4. **Generate report** with:
   - Summary of issues found
   - Priority ranking (Critical/High/Medium/Low)
   - Recommended fixes
   - Estimated time for each fix

5. **Create fix implementation plan** for identified issues

---

## âœ… **SUCCESS CRITERIA:**

```
Minimum acceptable:
â”œâ”€â”€ Pass rate: >85%
â”œâ”€â”€ Critical systems: 100% passing
â”œâ”€â”€ Performance: <10s for simple queries
â””â”€â”€ No security issues

Ideal state:
â”œâ”€â”€ Pass rate: >95%
â”œâ”€â”€ All systems: 100% passing
â”œâ”€â”€ Performance: <5s for simple queries
â””â”€â”€ Zero warnings
```

---

## ğŸ¯ **EXPECTED FINDINGS:**

Based on current issues, diagnostic will likely reveal:
- âŒ Nemotron reasoning trace cleaning (KNOWN)
- â“ Response performance issues
- â“ Memory integration gaps
- â“ Tool calling bottlenecks
- â“ Error handling weaknesses

---

**RUN THIS DIAGNOSTIC NOW TO GET COMPLETE HEALTH PICTURE!**
