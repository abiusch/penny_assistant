#!/usr/bin/env python3
"""
CRITICAL DEBUG TEST: Verify personality is ACTUALLY being applied to responses

This test simulates the full flow from user input to Penny response
to verify personality data affects the actual output.
"""

import sys
import os

# Add parent directory to path for imports
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from src.adapters.llm.openai_compat import OpenAICompatLLM

def test_personality_in_llm_adapter():
    """Test that LLM adapter uses personality prompts"""
    print("ğŸ§ª TESTING PERSONALITY IN LLM ADAPTER")
    print("=" * 70)

    # Create LLM adapter
    config = {
        "llm": {
            "base_url": "https://api.openai.com/v1",
            "api_key": os.getenv("OPENAI_API_KEY"),
            "model": "gpt-4",
            "temperature": 0.8,
            "max_tokens": 200
        }
    }

    llm = OpenAICompatLLM(config)

    # Test 1: Without personality (baseline)
    print("\nğŸ“‹ TEST 1: Without Personality (Baseline)")
    print("-" * 70)
    print("User: 'what up homie, explain async functions real quick'")
    print("System Prompt: Generic Penny")
    print("\nâ³ Generating response...")

    try:
        # This should use the NEW personality-aware system
        response1 = llm.complete("what up homie, explain async functions real quick", tone="helpful")
        print(f"\nâœ… Response:")
        print(response1)

        # Check if personality was applied
        if "ğŸ­ Personality-enhanced prompt applied" in str(response1):
            print("\nâœ… PERSONALITY WAS APPLIED!")
        else:
            print("\nâš ï¸ Checking console output for personality application...")

    except Exception as e:
        print(f"\nâŒ Error: {e}")
        import traceback
        traceback.print_exc()

    # Test 2: Verify prompt length changed
    print("\n\nğŸ“‹ TEST 2: Prompt Length Verification")
    print("-" * 70)
    print("A personality-enhanced prompt should be 300+ chars")
    print("A generic prompt is ~50 chars")
    print("\nâœ… If you saw 'ğŸ­ Personality-enhanced prompt applied' above,")
    print("   personality IS working!")

    print("\n" + "=" * 70)
    print("ğŸ¯ VERIFICATION CHECKLIST:")
    print("=" * 70)
    print("â–¡ Did you see 'ğŸ­ Personality-enhanced prompt applied'?")
    print("â–¡ Was the response casual and matched input energy?")
    print("â–¡ Did the response use contractions and casual language?")
    print("â–¡ Did the response have personality/sass?")
    print("\nIf YES to all: âœ… PERSONALITY IS WORKING!")
    print("If NO to any: âŒ Still debugging needed")


if __name__ == "__main__":
    test_personality_in_llm_adapter()
